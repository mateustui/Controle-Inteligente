{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Activation\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dados = pd.read_csv(r'G:\\Outros computadores\\Meu modelo Computador\\IFES\\9 Período\\Controle Inteligente\\Trabalho 3\\sem_pertu\\teste5.csv', on_bad_lines='skip', header=None)\n",
    "# Dados.values\n",
    "# Dados.head(5)\n",
    "# print(Dados)\n",
    "\n",
    "Entradas = Dados.iloc[:,:-1]\n",
    "Entradas.shape\n",
    "Saidas = Dados.iloc[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.164985</td>\n",
       "      <td>-4.285309</td>\n",
       "      <td>-0.005717</td>\n",
       "      <td>6.335611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.207838</td>\n",
       "      <td>-0.045053</td>\n",
       "      <td>0.057639</td>\n",
       "      <td>-0.025509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.208288</td>\n",
       "      <td>0.552431</td>\n",
       "      <td>0.057384</td>\n",
       "      <td>-0.911779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.202764</td>\n",
       "      <td>0.075944</td>\n",
       "      <td>0.048266</td>\n",
       "      <td>-0.189794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.202005</td>\n",
       "      <td>-0.043102</td>\n",
       "      <td>0.046368</td>\n",
       "      <td>-0.004341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49628</th>\n",
       "      <td>-0.000470</td>\n",
       "      <td>0.000573</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>0.000092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49629</th>\n",
       "      <td>-0.000464</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>0.000091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49630</th>\n",
       "      <td>-0.000458</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>-0.000076</td>\n",
       "      <td>0.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49631</th>\n",
       "      <td>-0.000453</td>\n",
       "      <td>0.000553</td>\n",
       "      <td>-0.000075</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49632</th>\n",
       "      <td>-0.000447</td>\n",
       "      <td>0.000546</td>\n",
       "      <td>-0.000074</td>\n",
       "      <td>0.000088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49633 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3\n",
       "0     -0.164985 -4.285309 -0.005717  6.335611\n",
       "1     -0.207838 -0.045053  0.057639 -0.025509\n",
       "2     -0.208288  0.552431  0.057384 -0.911779\n",
       "3     -0.202764  0.075944  0.048266 -0.189794\n",
       "4     -0.202005 -0.043102  0.046368 -0.004341\n",
       "...         ...       ...       ...       ...\n",
       "49628 -0.000470  0.000573 -0.000078  0.000092\n",
       "49629 -0.000464  0.000566 -0.000077  0.000091\n",
       "49630 -0.000458  0.000560 -0.000076  0.000090\n",
       "49631 -0.000453  0.000553 -0.000075  0.000089\n",
       "49632 -0.000447  0.000546 -0.000074  0.000088\n",
       "\n",
       "[49633 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Entradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest=train_test_split(Entradas,Saidas,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 420.62773990\n",
      "Iteration 2, loss = 419.74972273\n",
      "Iteration 3, loss = 418.91514061\n",
      "Iteration 4, loss = 417.91584451\n",
      "Iteration 5, loss = 416.63245583\n",
      "Iteration 6, loss = 414.97793793\n",
      "Iteration 7, loss = 413.00938622\n",
      "Iteration 8, loss = 410.78994702\n",
      "Iteration 9, loss = 408.16252774\n",
      "Iteration 10, loss = 405.26387451\n",
      "Iteration 11, loss = 402.14518155\n",
      "Iteration 12, loss = 398.56511820\n",
      "Iteration 13, loss = 394.73048943\n",
      "Iteration 14, loss = 390.67071831\n",
      "Iteration 15, loss = 386.57282119\n",
      "Iteration 16, loss = 382.15164196\n",
      "Iteration 17, loss = 377.22279518\n",
      "Iteration 18, loss = 372.39002507\n",
      "Iteration 19, loss = 367.24506512\n",
      "Iteration 20, loss = 362.06900043\n",
      "Iteration 21, loss = 356.81461970\n",
      "Iteration 22, loss = 351.24455996\n",
      "Iteration 23, loss = 345.94835020\n",
      "Iteration 24, loss = 340.23823829\n",
      "Iteration 25, loss = 334.78788131\n",
      "Iteration 26, loss = 329.20438168\n",
      "Iteration 27, loss = 323.83888678\n",
      "Iteration 28, loss = 318.22920123\n",
      "Iteration 29, loss = 312.84400326\n",
      "Iteration 30, loss = 307.45104948\n",
      "Iteration 31, loss = 302.31600106\n",
      "Iteration 32, loss = 297.02490795\n",
      "Iteration 33, loss = 291.94212953\n",
      "Iteration 34, loss = 286.96087305\n",
      "Iteration 35, loss = 282.05456360\n",
      "Iteration 36, loss = 277.45861684\n",
      "Iteration 37, loss = 272.86587355\n",
      "Iteration 38, loss = 268.49730483\n",
      "Iteration 39, loss = 264.48736219\n",
      "Iteration 40, loss = 260.70222835\n",
      "Iteration 41, loss = 257.27356977\n",
      "Iteration 42, loss = 254.07464581\n",
      "Iteration 43, loss = 251.00236531\n",
      "Iteration 44, loss = 248.24452031\n",
      "Iteration 45, loss = 245.78727358\n",
      "Iteration 46, loss = 243.48440760\n",
      "Iteration 47, loss = 241.35458600\n",
      "Iteration 48, loss = 239.31357191\n",
      "Iteration 49, loss = 237.59978446\n",
      "Iteration 50, loss = 236.03111236\n",
      "Iteration 51, loss = 234.61329415\n",
      "Iteration 52, loss = 233.21184904\n",
      "Iteration 53, loss = 231.88830578\n",
      "Iteration 54, loss = 230.74665832\n",
      "Iteration 55, loss = 229.60329569\n",
      "Iteration 56, loss = 228.41647004\n",
      "Iteration 57, loss = 227.41945003\n",
      "Iteration 58, loss = 226.53613407\n",
      "Iteration 59, loss = 225.61435813\n",
      "Iteration 60, loss = 224.57329819\n",
      "Iteration 61, loss = 223.62936209\n",
      "Iteration 62, loss = 222.58031079\n",
      "Iteration 63, loss = 221.72785365\n",
      "Iteration 64, loss = 221.01510596\n",
      "Iteration 65, loss = 220.42642539\n",
      "Iteration 66, loss = 219.91428357\n",
      "Iteration 67, loss = 219.51876349\n",
      "Iteration 68, loss = 219.11193717\n",
      "Iteration 69, loss = 218.80657427\n",
      "Iteration 70, loss = 218.57201596\n",
      "Iteration 71, loss = 218.35452388\n",
      "Iteration 72, loss = 218.10742159\n",
      "Iteration 73, loss = 218.02162835\n",
      "Iteration 74, loss = 217.82690702\n",
      "Iteration 75, loss = 217.68454387\n",
      "Iteration 76, loss = 217.53486346\n",
      "Iteration 77, loss = 217.42514222\n",
      "Iteration 78, loss = 217.30236638\n",
      "Iteration 79, loss = 217.20860561\n",
      "Iteration 80, loss = 217.06759165\n",
      "Iteration 81, loss = 217.05556376\n",
      "Iteration 82, loss = 216.90462050\n",
      "Iteration 83, loss = 216.84623042\n",
      "Iteration 84, loss = 216.74623632\n",
      "Iteration 85, loss = 216.66005713\n",
      "Iteration 86, loss = 216.57797642\n",
      "Iteration 87, loss = 216.44399513\n",
      "Iteration 88, loss = 216.35088237\n",
      "Iteration 89, loss = 216.28416194\n",
      "Iteration 90, loss = 216.19301856\n",
      "Iteration 91, loss = 216.15126537\n",
      "Iteration 92, loss = 216.04481858\n",
      "Iteration 93, loss = 215.98868423\n",
      "Iteration 94, loss = 215.89913366\n",
      "Iteration 95, loss = 215.84939707\n",
      "Iteration 96, loss = 215.76414057\n",
      "Iteration 97, loss = 215.69419307\n",
      "Iteration 98, loss = 215.65310911\n",
      "Iteration 99, loss = 215.57200574\n",
      "Iteration 100, loss = 215.51989338\n",
      "Iteration 101, loss = 215.45344918\n",
      "Iteration 102, loss = 215.41303295\n",
      "Iteration 103, loss = 215.33033587\n",
      "Iteration 104, loss = 215.29912716\n",
      "Iteration 105, loss = 215.22635476\n",
      "Iteration 106, loss = 215.17092850\n",
      "Iteration 107, loss = 215.10425738\n",
      "Iteration 108, loss = 215.06466057\n",
      "Iteration 109, loss = 215.01037734\n",
      "Iteration 110, loss = 214.97466000\n",
      "Iteration 111, loss = 214.89346581\n",
      "Iteration 112, loss = 214.83566496\n",
      "Iteration 113, loss = 214.81445396\n",
      "Iteration 114, loss = 214.74082582\n",
      "Iteration 115, loss = 214.69353267\n",
      "Iteration 116, loss = 214.65213262\n",
      "Iteration 117, loss = 214.62443871\n",
      "Iteration 118, loss = 214.54235916\n",
      "Iteration 119, loss = 214.50830757\n",
      "Iteration 120, loss = 214.45025222\n",
      "Iteration 121, loss = 214.38566393\n",
      "Iteration 122, loss = 214.33232857\n",
      "Iteration 123, loss = 214.28558215\n",
      "Iteration 124, loss = 214.22006203\n",
      "Iteration 125, loss = 214.17825826\n",
      "Iteration 126, loss = 214.12643011\n",
      "Iteration 127, loss = 214.06234578\n",
      "Iteration 128, loss = 214.02274214\n",
      "Iteration 129, loss = 213.97229426\n",
      "Iteration 130, loss = 213.91868688\n",
      "Iteration 131, loss = 213.87437522\n",
      "Iteration 132, loss = 213.85908464\n",
      "Iteration 133, loss = 213.78719320\n",
      "Iteration 134, loss = 213.74073313\n",
      "Iteration 135, loss = 213.67474351\n",
      "Iteration 136, loss = 213.63456088\n",
      "Iteration 137, loss = 213.57733655\n",
      "Iteration 138, loss = 213.53068625\n",
      "Iteration 139, loss = 213.48757117\n",
      "Iteration 140, loss = 213.46244397\n",
      "Iteration 141, loss = 213.38238775\n",
      "Iteration 142, loss = 213.31965961\n",
      "Iteration 143, loss = 213.28896505\n",
      "Iteration 144, loss = 213.25284869\n",
      "Iteration 145, loss = 213.18776688\n",
      "Iteration 146, loss = 213.16838391\n",
      "Iteration 147, loss = 213.09196174\n",
      "Iteration 148, loss = 213.06610338\n",
      "Iteration 149, loss = 213.01194386\n",
      "Iteration 150, loss = 212.94568275\n",
      "Iteration 151, loss = 212.91173696\n",
      "Iteration 152, loss = 212.88850835\n",
      "Iteration 153, loss = 212.82338070\n",
      "Iteration 154, loss = 212.78061972\n",
      "Iteration 155, loss = 212.72277793\n",
      "Iteration 156, loss = 212.67387065\n",
      "Iteration 157, loss = 212.61761656\n",
      "Iteration 158, loss = 212.58511566\n",
      "Iteration 159, loss = 212.59064733\n",
      "Iteration 160, loss = 212.52522315\n",
      "Iteration 161, loss = 212.48169104\n",
      "Iteration 162, loss = 212.45371131\n",
      "Iteration 163, loss = 212.39956598\n",
      "Iteration 164, loss = 212.34730012\n",
      "Iteration 165, loss = 212.35220534\n",
      "Iteration 166, loss = 212.29386863\n",
      "Iteration 167, loss = 212.26974031\n",
      "Iteration 168, loss = 212.24368267\n",
      "Iteration 169, loss = 212.20568413\n",
      "Iteration 170, loss = 212.16224170\n",
      "Iteration 171, loss = 212.15918333\n",
      "Iteration 172, loss = 212.13205696\n",
      "Iteration 173, loss = 212.10235576\n",
      "Iteration 174, loss = 212.06514909\n",
      "Iteration 175, loss = 212.02153054\n",
      "Iteration 176, loss = 211.99028160\n",
      "Iteration 177, loss = 212.02109081\n",
      "Iteration 178, loss = 211.96151948\n",
      "Iteration 179, loss = 211.91878847\n",
      "Iteration 180, loss = 211.91347364\n",
      "Iteration 181, loss = 211.87094370\n",
      "Iteration 182, loss = 211.86054155\n",
      "Iteration 183, loss = 211.82270684\n",
      "Iteration 184, loss = 211.76028289\n",
      "Iteration 185, loss = 211.75837266\n",
      "Iteration 186, loss = 211.73387228\n",
      "Iteration 187, loss = 211.71484030\n",
      "Iteration 188, loss = 211.68648846\n",
      "Iteration 189, loss = 211.68516631\n",
      "Iteration 190, loss = 211.63857199\n",
      "Iteration 191, loss = 211.61830200\n",
      "Iteration 192, loss = 211.61873865\n",
      "Iteration 193, loss = 211.54058798\n",
      "Iteration 194, loss = 211.52294318\n",
      "Iteration 195, loss = 211.48453792\n",
      "Iteration 196, loss = 211.44760500\n",
      "Iteration 197, loss = 211.38999031\n",
      "Iteration 198, loss = 211.38528125\n",
      "Iteration 199, loss = 211.33785391\n",
      "Iteration 200, loss = 211.30781206\n",
      "Iteration 201, loss = 211.29229802\n",
      "Iteration 202, loss = 211.29881398\n",
      "Iteration 203, loss = 211.20182392\n",
      "Iteration 204, loss = 211.18720403\n",
      "Iteration 205, loss = 211.17904222\n",
      "Iteration 206, loss = 211.14429689\n",
      "Iteration 207, loss = 211.07554288\n",
      "Iteration 208, loss = 211.03794354\n",
      "Iteration 209, loss = 211.02135042\n",
      "Iteration 210, loss = 210.98035590\n",
      "Iteration 211, loss = 210.97296219\n",
      "Iteration 212, loss = 210.93073594\n",
      "Iteration 213, loss = 210.89892601\n",
      "Iteration 214, loss = 210.88232581\n",
      "Iteration 215, loss = 210.85632019\n",
      "Iteration 216, loss = 210.83316742\n",
      "Iteration 217, loss = 210.81115020\n",
      "Iteration 218, loss = 210.76521044\n",
      "Iteration 219, loss = 210.76164911\n",
      "Iteration 220, loss = 210.72503865\n",
      "Iteration 221, loss = 210.69790297\n",
      "Iteration 222, loss = 210.66727837\n",
      "Iteration 223, loss = 210.67042407\n",
      "Iteration 224, loss = 210.67203800\n",
      "Iteration 225, loss = 210.63457640\n",
      "Iteration 226, loss = 210.62617399\n",
      "Iteration 227, loss = 210.58435124\n",
      "Iteration 228, loss = 210.56037044\n",
      "Iteration 229, loss = 210.48007054\n",
      "Iteration 230, loss = 210.42840169\n",
      "Iteration 231, loss = 210.32207278\n",
      "Iteration 232, loss = 210.33215974\n",
      "Iteration 233, loss = 210.22535041\n",
      "Iteration 234, loss = 210.18990623\n",
      "Iteration 235, loss = 210.16950614\n",
      "Iteration 236, loss = 210.14441320\n",
      "Iteration 237, loss = 210.12017307\n",
      "Iteration 238, loss = 210.08785560\n",
      "Iteration 239, loss = 210.08803734\n",
      "Iteration 240, loss = 210.00370161\n",
      "Iteration 241, loss = 209.99836438\n",
      "Iteration 242, loss = 209.96218677\n",
      "Iteration 243, loss = 209.91918334\n",
      "Iteration 244, loss = 209.89002306\n",
      "Iteration 245, loss = 209.86396911\n",
      "Iteration 246, loss = 209.84478145\n",
      "Iteration 247, loss = 209.81545997\n",
      "Iteration 248, loss = 209.79652809\n",
      "Iteration 249, loss = 209.75325469\n",
      "Iteration 250, loss = 209.70946436\n",
      "Iteration 251, loss = 209.69572359\n",
      "Iteration 252, loss = 209.65730722\n",
      "Iteration 253, loss = 209.64384905\n",
      "Iteration 254, loss = 209.57447522\n",
      "Iteration 255, loss = 209.57774898\n",
      "Iteration 256, loss = 209.50952603\n",
      "Iteration 257, loss = 209.49529952\n",
      "Iteration 258, loss = 209.44356290\n",
      "Iteration 259, loss = 209.37805875\n",
      "Iteration 260, loss = 209.39091500\n",
      "Iteration 261, loss = 209.30855326\n",
      "Iteration 262, loss = 209.27813930\n",
      "Iteration 263, loss = 209.23872318\n",
      "Iteration 264, loss = 209.21346499\n",
      "Iteration 265, loss = 209.17838625\n",
      "Iteration 266, loss = 209.16863554\n",
      "Iteration 267, loss = 209.13359557\n",
      "Iteration 268, loss = 209.12078846\n",
      "Iteration 269, loss = 209.06738173\n",
      "Iteration 270, loss = 209.02259644\n",
      "Iteration 271, loss = 208.98756608\n",
      "Iteration 272, loss = 208.96788706\n",
      "Iteration 273, loss = 208.92431873\n",
      "Iteration 274, loss = 208.91964439\n",
      "Iteration 275, loss = 208.89088920\n",
      "Iteration 276, loss = 208.87200769\n",
      "Iteration 277, loss = 208.84237863\n",
      "Iteration 278, loss = 208.76597583\n",
      "Iteration 279, loss = 208.79202725\n",
      "Iteration 280, loss = 208.78288143\n",
      "Iteration 281, loss = 208.70630579\n",
      "Iteration 282, loss = 208.71601968\n",
      "Iteration 283, loss = 208.66364725\n",
      "Iteration 284, loss = 208.63526437\n",
      "Iteration 285, loss = 208.61779722\n",
      "Iteration 286, loss = 208.57834157\n",
      "Iteration 287, loss = 208.57839086\n",
      "Iteration 288, loss = 208.56960195\n",
      "Iteration 289, loss = 208.52107021\n",
      "Iteration 290, loss = 208.47655429\n",
      "Iteration 291, loss = 208.48139641\n",
      "Iteration 292, loss = 208.46530654\n",
      "Iteration 293, loss = 208.43147923\n",
      "Iteration 294, loss = 208.40658730\n",
      "Iteration 295, loss = 208.40223265\n",
      "Iteration 296, loss = 208.35884896\n",
      "Iteration 297, loss = 208.33665280\n",
      "Iteration 298, loss = 208.29050558\n",
      "Iteration 299, loss = 208.29923809\n",
      "Iteration 300, loss = 208.25438493\n",
      "Iteration 301, loss = 208.24771265\n",
      "Iteration 302, loss = 208.22218711\n",
      "Iteration 303, loss = 208.19125611\n",
      "Iteration 304, loss = 208.18196812\n",
      "Iteration 305, loss = 208.24046074\n",
      "Iteration 306, loss = 208.13504714\n",
      "Iteration 307, loss = 208.09930159\n",
      "Iteration 308, loss = 208.11520392\n",
      "Iteration 309, loss = 208.08989720\n",
      "Iteration 310, loss = 208.06091165\n",
      "Iteration 311, loss = 208.01529457\n",
      "Iteration 312, loss = 208.02926828\n",
      "Iteration 313, loss = 208.04010762\n",
      "Iteration 314, loss = 207.98479700\n",
      "Iteration 315, loss = 207.96363535\n",
      "Iteration 316, loss = 207.95761991\n",
      "Iteration 317, loss = 207.97191114\n",
      "Iteration 318, loss = 207.95182734\n",
      "Iteration 319, loss = 207.88291406\n",
      "Iteration 320, loss = 207.88361873\n",
      "Iteration 321, loss = 207.86602742\n",
      "Iteration 322, loss = 207.83197866\n",
      "Iteration 323, loss = 207.85853464\n",
      "Iteration 324, loss = 207.80770149\n",
      "Iteration 325, loss = 207.77602557\n",
      "Iteration 326, loss = 207.78912294\n",
      "Iteration 327, loss = 207.76678323\n",
      "Iteration 328, loss = 207.75512317\n",
      "Iteration 329, loss = 207.71352074\n",
      "Iteration 330, loss = 207.71503898\n",
      "Iteration 331, loss = 207.74187019\n",
      "Iteration 332, loss = 207.69543433\n",
      "Iteration 333, loss = 207.73701735\n",
      "Iteration 334, loss = 207.65484674\n",
      "Iteration 335, loss = 207.64865203\n",
      "Iteration 336, loss = 207.64672044\n",
      "Iteration 337, loss = 207.64410713\n",
      "Iteration 338, loss = 207.63027605\n",
      "Iteration 339, loss = 207.63282634\n",
      "Iteration 340, loss = 207.59753619\n",
      "Iteration 341, loss = 207.60124628\n",
      "Iteration 342, loss = 207.57810654\n",
      "Iteration 343, loss = 207.55503549\n",
      "Iteration 344, loss = 207.53943725\n",
      "Iteration 345, loss = 207.52717037\n",
      "Iteration 346, loss = 207.52403708\n",
      "Iteration 347, loss = 207.53360418\n",
      "Iteration 348, loss = 207.48535681\n",
      "Iteration 349, loss = 207.50082234\n",
      "Iteration 350, loss = 207.46906844\n",
      "Iteration 351, loss = 207.49854617\n",
      "Iteration 352, loss = 207.42366161\n",
      "Iteration 353, loss = 207.46345891\n",
      "Iteration 354, loss = 207.46029734\n",
      "Iteration 355, loss = 207.42945377\n",
      "Iteration 356, loss = 207.41205878\n",
      "Iteration 357, loss = 207.38601900\n",
      "Iteration 358, loss = 207.40952890\n",
      "Iteration 359, loss = 207.37281568\n",
      "Iteration 360, loss = 207.35726826\n",
      "Iteration 361, loss = 207.33294607\n",
      "Iteration 362, loss = 207.36107597\n",
      "Iteration 363, loss = 207.39138163\n",
      "Iteration 364, loss = 207.30925259\n",
      "Iteration 365, loss = 207.30038867\n",
      "Iteration 366, loss = 207.34770067\n",
      "Iteration 367, loss = 207.28886589\n",
      "Iteration 368, loss = 207.27333557\n",
      "Iteration 369, loss = 207.28838415\n",
      "Iteration 370, loss = 207.30645369\n",
      "Iteration 371, loss = 207.29860981\n",
      "Iteration 372, loss = 207.24462836\n",
      "Iteration 373, loss = 207.21043204\n",
      "Iteration 374, loss = 207.25244977\n",
      "Iteration 375, loss = 207.19006877\n",
      "Iteration 376, loss = 207.20703558\n",
      "Iteration 377, loss = 207.22083081\n",
      "Iteration 378, loss = 207.21803620\n",
      "Iteration 379, loss = 207.15026015\n",
      "Iteration 380, loss = 207.19986886\n",
      "Iteration 381, loss = 207.15566584\n",
      "Iteration 382, loss = 207.23337783\n",
      "Iteration 383, loss = 207.15854879\n",
      "Iteration 384, loss = 207.15803720\n",
      "Iteration 385, loss = 207.15539337\n",
      "Iteration 386, loss = 207.12920342\n",
      "Iteration 387, loss = 207.15639828\n",
      "Iteration 388, loss = 207.10845053\n",
      "Iteration 389, loss = 207.13662660\n",
      "Iteration 390, loss = 207.11977402\n",
      "Iteration 391, loss = 207.12429071\n",
      "Iteration 392, loss = 207.14928261\n",
      "Iteration 393, loss = 207.09308798\n",
      "Iteration 394, loss = 207.06246271\n",
      "Iteration 395, loss = 207.06340955\n",
      "Iteration 396, loss = 207.05305243\n",
      "Iteration 397, loss = 207.07637022\n",
      "Iteration 398, loss = 207.07024309\n",
      "Iteration 399, loss = 207.07113878\n",
      "Iteration 400, loss = 207.00457923\n",
      "Iteration 401, loss = 207.03874747\n",
      "Iteration 402, loss = 207.01801906\n",
      "Iteration 403, loss = 207.01217660\n",
      "Iteration 404, loss = 207.00103454\n",
      "Iteration 405, loss = 206.94845639\n",
      "Iteration 406, loss = 206.97647597\n",
      "Iteration 407, loss = 206.96530425\n",
      "Iteration 408, loss = 206.91637138\n",
      "Iteration 409, loss = 206.97476858\n",
      "Iteration 410, loss = 206.89679596\n",
      "Iteration 411, loss = 206.91811466\n",
      "Iteration 412, loss = 206.89253143\n",
      "Iteration 413, loss = 206.93970900\n",
      "Iteration 414, loss = 206.93706390\n",
      "Iteration 415, loss = 206.90320443\n",
      "Iteration 416, loss = 206.85287601\n",
      "Iteration 417, loss = 206.83835926\n",
      "Iteration 418, loss = 206.86752115\n",
      "Iteration 419, loss = 206.86020481\n",
      "Iteration 420, loss = 206.83678490\n",
      "Iteration 421, loss = 206.84837503\n",
      "Iteration 422, loss = 206.79559237\n",
      "Iteration 423, loss = 206.82419115\n",
      "Iteration 424, loss = 206.82277121\n",
      "Iteration 425, loss = 206.81765138\n",
      "Iteration 426, loss = 206.76587543\n",
      "Iteration 427, loss = 206.77475498\n",
      "Iteration 428, loss = 206.78215256\n",
      "Iteration 429, loss = 206.75583018\n",
      "Iteration 430, loss = 206.74123324\n",
      "Iteration 431, loss = 206.76947144\n",
      "Iteration 432, loss = 206.77556140\n",
      "Iteration 433, loss = 206.71442273\n",
      "Iteration 434, loss = 206.71506525\n",
      "Iteration 435, loss = 206.68496917\n",
      "Iteration 436, loss = 206.65910695\n",
      "Iteration 437, loss = 206.69001288\n",
      "Iteration 438, loss = 206.66180623\n",
      "Iteration 439, loss = 206.65445631\n",
      "Iteration 440, loss = 206.67992078\n",
      "Iteration 441, loss = 206.66274735\n",
      "Iteration 442, loss = 206.64726532\n",
      "Iteration 443, loss = 206.63502519\n",
      "Iteration 444, loss = 206.64808754\n",
      "Iteration 445, loss = 206.59998102\n",
      "Iteration 446, loss = 206.61869577\n",
      "Iteration 447, loss = 206.59708563\n",
      "Iteration 448, loss = 206.59433157\n",
      "Iteration 449, loss = 206.56340542\n",
      "Iteration 450, loss = 206.59490897\n",
      "Iteration 451, loss = 206.58395371\n",
      "Iteration 452, loss = 206.56388681\n",
      "Iteration 453, loss = 206.57373346\n",
      "Iteration 454, loss = 206.54293586\n",
      "Iteration 455, loss = 206.56914979\n",
      "Iteration 456, loss = 206.52277858\n",
      "Iteration 457, loss = 206.52461262\n",
      "Iteration 458, loss = 206.56447222\n",
      "Iteration 459, loss = 206.55331557\n",
      "Iteration 460, loss = 206.56351195\n",
      "Iteration 461, loss = 206.52729176\n",
      "Iteration 462, loss = 206.49197822\n",
      "Iteration 463, loss = 206.49986947\n",
      "Iteration 464, loss = 206.51445867\n",
      "Iteration 465, loss = 206.49448908\n",
      "Iteration 466, loss = 206.47219692\n",
      "Iteration 467, loss = 206.46989356\n",
      "Iteration 468, loss = 206.50086246\n",
      "Iteration 469, loss = 206.45493110\n",
      "Iteration 470, loss = 206.50272115\n",
      "Iteration 471, loss = 206.43446030\n",
      "Iteration 472, loss = 206.47204749\n",
      "Iteration 473, loss = 206.46935795\n",
      "Iteration 474, loss = 206.40513922\n",
      "Iteration 475, loss = 206.44869261\n",
      "Iteration 476, loss = 206.49471429\n",
      "Iteration 477, loss = 206.40494930\n",
      "Iteration 478, loss = 206.39338312\n",
      "Iteration 479, loss = 206.45029733\n",
      "Iteration 480, loss = 206.48595481\n",
      "Iteration 481, loss = 206.38606331\n",
      "Iteration 482, loss = 206.40041218\n",
      "Iteration 483, loss = 206.40503666\n",
      "Iteration 484, loss = 206.36735122\n",
      "Iteration 485, loss = 206.36477698\n",
      "Iteration 486, loss = 206.40437259\n",
      "Iteration 487, loss = 206.35927538\n",
      "Iteration 488, loss = 206.38366994\n",
      "Iteration 489, loss = 206.34469090\n",
      "Iteration 490, loss = 206.36527325\n",
      "Iteration 491, loss = 206.31963793\n",
      "Iteration 492, loss = 206.35607562\n",
      "Iteration 493, loss = 206.30344045\n",
      "Iteration 494, loss = 206.32919130\n",
      "Iteration 495, loss = 206.32988370\n",
      "Iteration 496, loss = 206.37385929\n",
      "Iteration 497, loss = 206.30108810\n",
      "Iteration 498, loss = 206.30101215\n",
      "Iteration 499, loss = 206.34515492\n",
      "Iteration 500, loss = 206.34660081\n",
      "Iteration 501, loss = 206.30087757\n",
      "Iteration 502, loss = 206.29651503\n",
      "Iteration 503, loss = 206.30632292\n",
      "Iteration 504, loss = 206.27480415\n",
      "Iteration 505, loss = 206.25295808\n",
      "Iteration 506, loss = 206.28098779\n",
      "Iteration 507, loss = 206.25654530\n",
      "Iteration 508, loss = 206.28084227\n",
      "Iteration 509, loss = 206.28375082\n",
      "Iteration 510, loss = 206.29375728\n",
      "Iteration 511, loss = 206.25168648\n",
      "Iteration 512, loss = 206.23865075\n",
      "Iteration 513, loss = 206.24676413\n",
      "Iteration 514, loss = 206.23583722\n",
      "Iteration 515, loss = 206.22303601\n",
      "Iteration 516, loss = 206.23592634\n",
      "Iteration 517, loss = 206.22554996\n",
      "Iteration 518, loss = 206.19835859\n",
      "Iteration 519, loss = 206.20547525\n",
      "Iteration 520, loss = 206.28015067\n",
      "Iteration 521, loss = 206.24225847\n",
      "Iteration 522, loss = 206.16153805\n",
      "Iteration 523, loss = 206.18352436\n",
      "Iteration 524, loss = 206.19586988\n",
      "Iteration 525, loss = 206.18420404\n",
      "Iteration 526, loss = 206.17805814\n",
      "Iteration 527, loss = 206.17189898\n",
      "Iteration 528, loss = 206.18822750\n",
      "Iteration 529, loss = 206.15415262\n",
      "Iteration 530, loss = 206.16700770\n",
      "Iteration 531, loss = 206.19382565\n",
      "Iteration 532, loss = 206.14305091\n",
      "Iteration 533, loss = 206.13786134\n",
      "Iteration 534, loss = 206.12998724\n",
      "Iteration 535, loss = 206.11554896\n",
      "Iteration 536, loss = 206.15376088\n",
      "Iteration 537, loss = 206.13114251\n",
      "Iteration 538, loss = 206.11884120\n",
      "Iteration 539, loss = 206.09711402\n",
      "Iteration 540, loss = 206.10485146\n",
      "Iteration 541, loss = 206.05927699\n",
      "Iteration 542, loss = 206.12561663\n",
      "Iteration 543, loss = 206.12776615\n",
      "Iteration 544, loss = 206.09317839\n",
      "Iteration 545, loss = 206.11152255\n",
      "Iteration 546, loss = 206.08371587\n",
      "Iteration 547, loss = 206.06254426\n",
      "Iteration 548, loss = 206.07303507\n",
      "Iteration 549, loss = 206.09092856\n",
      "Iteration 550, loss = 206.08093621\n",
      "Iteration 551, loss = 206.05698894\n",
      "Iteration 552, loss = 206.07605402\n",
      "Iteration 553, loss = 206.07115838\n",
      "Iteration 554, loss = 206.05198236\n",
      "Iteration 555, loss = 206.10387887\n",
      "Iteration 556, loss = 206.09522090\n",
      "Iteration 557, loss = 206.05239931\n",
      "Iteration 558, loss = 206.04173190\n",
      "Iteration 559, loss = 205.98707074\n",
      "Iteration 560, loss = 206.04282996\n",
      "Iteration 561, loss = 206.06766507\n",
      "Iteration 562, loss = 206.00899942\n",
      "Iteration 563, loss = 205.99039315\n",
      "Iteration 564, loss = 206.06394989\n",
      "Iteration 565, loss = 206.00811207\n",
      "Iteration 566, loss = 206.03314315\n",
      "Iteration 567, loss = 205.96760562\n",
      "Iteration 568, loss = 206.04776847\n",
      "Iteration 569, loss = 205.99119399\n",
      "Iteration 570, loss = 206.02191272\n",
      "Iteration 571, loss = 206.02144651\n",
      "Iteration 572, loss = 205.98917820\n",
      "Iteration 573, loss = 205.95629618\n",
      "Iteration 574, loss = 205.95286544\n",
      "Iteration 575, loss = 206.00209093\n",
      "Iteration 576, loss = 206.00382411\n",
      "Iteration 577, loss = 205.97937742\n",
      "Iteration 578, loss = 205.97130692\n",
      "Iteration 579, loss = 205.98114453\n",
      "Iteration 580, loss = 205.99712192\n",
      "Iteration 581, loss = 205.94964898\n",
      "Iteration 582, loss = 205.96814449\n",
      "Iteration 583, loss = 205.89763279\n",
      "Iteration 584, loss = 205.92374590\n",
      "Iteration 585, loss = 205.90342301\n",
      "Iteration 586, loss = 205.95203280\n",
      "Iteration 587, loss = 205.90838229\n",
      "Iteration 588, loss = 205.94314542\n",
      "Iteration 589, loss = 205.95008462\n",
      "Iteration 590, loss = 205.92932972\n",
      "Iteration 591, loss = 205.93991996\n",
      "Iteration 592, loss = 205.98728914\n",
      "Iteration 593, loss = 205.85848986\n",
      "Iteration 594, loss = 205.89897618\n",
      "Iteration 595, loss = 205.90749870\n",
      "Iteration 596, loss = 205.88988742\n",
      "Iteration 597, loss = 205.89517373\n",
      "Iteration 598, loss = 205.93040050\n",
      "Iteration 599, loss = 205.93015049\n",
      "Iteration 600, loss = 205.83703916\n",
      "Iteration 601, loss = 205.86013386\n",
      "Iteration 602, loss = 205.86960300\n",
      "Iteration 603, loss = 205.86503151\n",
      "Iteration 604, loss = 205.88273750\n",
      "Iteration 605, loss = 205.88202919\n",
      "Iteration 606, loss = 205.83991452\n",
      "Iteration 607, loss = 205.82381149\n",
      "Iteration 608, loss = 205.90156010\n",
      "Iteration 609, loss = 205.82746027\n",
      "Iteration 610, loss = 205.87518798\n",
      "Iteration 611, loss = 205.86510702\n",
      "Iteration 612, loss = 205.86824903\n",
      "Iteration 613, loss = 205.88218500\n",
      "Iteration 614, loss = 205.84808030\n",
      "Iteration 615, loss = 205.84811767\n",
      "Iteration 616, loss = 205.80612069\n",
      "Iteration 617, loss = 205.80435881\n",
      "Iteration 618, loss = 205.81813019\n",
      "Iteration 619, loss = 205.83818577\n",
      "Iteration 620, loss = 205.78240665\n",
      "Iteration 621, loss = 205.82913101\n",
      "Iteration 622, loss = 205.77937720\n",
      "Iteration 623, loss = 205.79445485\n",
      "Iteration 624, loss = 205.82876512\n",
      "Iteration 625, loss = 205.75500344\n",
      "Iteration 626, loss = 205.82103639\n",
      "Iteration 627, loss = 205.79290205\n",
      "Iteration 628, loss = 205.82698213\n",
      "Iteration 629, loss = 205.80960098\n",
      "Iteration 630, loss = 205.80966709\n",
      "Iteration 631, loss = 205.80339968\n",
      "Iteration 632, loss = 205.79399932\n",
      "Iteration 633, loss = 205.80734309\n",
      "Iteration 634, loss = 205.74411138\n",
      "Iteration 635, loss = 205.73583557\n",
      "Iteration 636, loss = 205.78796336\n",
      "Iteration 637, loss = 205.71427728\n",
      "Iteration 638, loss = 205.75401340\n",
      "Iteration 639, loss = 205.74812900\n",
      "Iteration 640, loss = 205.73253744\n",
      "Iteration 641, loss = 205.79533261\n",
      "Iteration 642, loss = 205.76881814\n",
      "Iteration 643, loss = 205.74264320\n",
      "Iteration 644, loss = 205.71858308\n",
      "Iteration 645, loss = 205.73271119\n",
      "Iteration 646, loss = 205.72113633\n",
      "Iteration 647, loss = 205.68879563\n",
      "Iteration 648, loss = 205.71921467\n",
      "Iteration 649, loss = 205.70390181\n",
      "Iteration 650, loss = 205.73322285\n",
      "Iteration 651, loss = 205.68685649\n",
      "Iteration 652, loss = 205.72762768\n",
      "Iteration 653, loss = 205.67192505\n",
      "Iteration 654, loss = 205.66703934\n",
      "Iteration 655, loss = 205.69328167\n",
      "Iteration 656, loss = 205.67710673\n",
      "Iteration 657, loss = 205.66685929\n",
      "Iteration 658, loss = 205.65426891\n",
      "Iteration 659, loss = 205.72744194\n",
      "Iteration 660, loss = 205.67841031\n",
      "Iteration 661, loss = 205.70169424\n",
      "Iteration 662, loss = 205.63949501\n",
      "Iteration 663, loss = 205.63820603\n",
      "Iteration 664, loss = 205.66176794\n",
      "Iteration 665, loss = 205.68267601\n",
      "Iteration 666, loss = 205.61571052\n",
      "Iteration 667, loss = 205.61344222\n",
      "Iteration 668, loss = 205.65222896\n",
      "Iteration 669, loss = 205.65725705\n",
      "Iteration 670, loss = 205.67323640\n",
      "Iteration 671, loss = 205.65690058\n",
      "Iteration 672, loss = 205.62919531\n",
      "Iteration 673, loss = 205.65722290\n",
      "Iteration 674, loss = 205.63836726\n",
      "Iteration 675, loss = 205.62189295\n",
      "Iteration 676, loss = 205.63897282\n",
      "Iteration 677, loss = 205.57999136\n",
      "Iteration 678, loss = 205.56688706\n",
      "Iteration 679, loss = 205.56347367\n",
      "Iteration 680, loss = 205.61378169\n",
      "Iteration 681, loss = 205.60179640\n",
      "Iteration 682, loss = 205.62603308\n",
      "Iteration 683, loss = 205.59103285\n",
      "Iteration 684, loss = 205.61847484\n",
      "Iteration 685, loss = 205.59841526\n",
      "Iteration 686, loss = 205.52974498\n",
      "Iteration 687, loss = 205.61537878\n",
      "Iteration 688, loss = 205.58392669\n",
      "Iteration 689, loss = 205.58485001\n",
      "Iteration 690, loss = 205.57504524\n",
      "Iteration 691, loss = 205.54185259\n",
      "Iteration 692, loss = 205.56546826\n",
      "Iteration 693, loss = 205.54576657\n",
      "Iteration 694, loss = 205.58719543\n",
      "Iteration 695, loss = 205.53424201\n",
      "Iteration 696, loss = 205.54542637\n",
      "Iteration 697, loss = 205.55692977\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPRegressor(hidden_layer_sizes=[4, 2], max_iter=10000, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPRegressor</label><div class=\"sk-toggleable__content\"><pre>MLPRegressor(hidden_layer_sizes=[4, 2], max_iter=10000, verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPRegressor(hidden_layer_sizes=[4, 2], max_iter=10000, verbose=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rede=MLPRegressor(hidden_layer_sizes=[4,2],\n",
    "                  activation='relu',\n",
    "                  verbose=True,\n",
    "                  max_iter=10000,\n",
    "                  solver=\"adam\")\n",
    "Rede.fit(Entradas,Saidas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score do treinamento:  0.47047731658472636\n",
      "R2 Score do teste:  0.6753990308949154\n"
     ]
    }
   ],
   "source": [
    "r2train=Rede.score(Xtrain, Ytrain)\n",
    "print(\"R2 Score do treinamento: \", r2train)\n",
    "r2test=Rede.score(Xtest, Ytest)\n",
    "print(\"R2 Score do teste: \", r2test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD8CAYAAACCRVh7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAZJ0lEQVR4nO3df5Bdd3nf8ffjZXHX5sdasIC1trDsGrm4SiS8Y0hVaMAmMk7AslNALqFuwkTQgZlQGg0WZhK3Y2oHYaCdNGTkxBMnNcamFsKTpBE2kNBSDEiWsGRsxZYxoJViC1wBE+8IafX0jz1Xvru+Z3/cvef+2H2/Znb23u+5595nz713P+d8z/ecE5mJJEmNnNLpAiRJ3cuQkCSVMiQkSaUMCUlSKUNCklTKkJAklWpJSETErRHxVETsrWtbEhH3RsSjxe8z6qZtiojHImJfRKxtRQ2SpNZr1ZbEnwGXTWm7FvhyZp4PfLm4T0S8GlgPXFjM80cR0deiOiRJLdSSkMjMrwFPT2m+ArituH0bsK6u/XOZeTQzvwc8BlzcijokSa31vAqf++WZeQggMw9FxMuK9mHg/rrHHSjaniMiNgAbAE4//fSLLrjgggrLlaSFZ+fOnT/KzKFm568yJMpEg7aG5wbJzC3AFoCRkZHcsWNHlXVJ0oITEd+fz/xVjm56MiLOBCh+P1W0HwDOrnvcWcDBCuuQJDWpypC4B7imuH0N8MW69vURcWpELAfOB75VYR2SpCa1pLspIu4Afhl4aUQcAH4fuAm4KyLeA/wAeDtAZj4UEXcB3wWOA+/PzPFW1CFJaq2WhERmXl0y6ZKSx38M+FgrXluSVB2PuJYklTIkJEmlDAlJUilDQpJUypCQJJUyJCRJpQwJSVIpQ0KSVMqQkCSVMiQkSaUMCUlSKUNCklTKkJAklTIkJEmlDAlJUilDQpJUypCQJJUyJCRJpVpy+dIyEbECuLOu6Vzg94BB4LeBw0X7RzLzr6usRZI0d5WGRGbuA1YBREQfMAp8AfhN4FOZ+YkqX1+SND/t7G66BNifmd9v42tKkuahnSGxHrij7v4HIuLBiLg1Is5oYx2SpFlqS0hExPOBtwGfL5o+A5zHRFfUIeDmkvk2RMSOiNhx+PDhRg+RJFWoXVsSbwEeyMwnATLzycwcz8wTwC3AxY1myswtmTmSmSNDQ0NtKlWSVNOukLiauq6miDizbtqVwN421SFJmoNKRzcBRMRpwJuB99Y1fzwiVgEJPDFlmiSpS1QeEpn5DPCSKW3vrvp1JUnz5xHXkqRShoQkqZQhIUkqZUhIkkoZEpKkUoaEJKmUISFJKmVISJJKGRKSpFKGhCSplCEhSSplSEiSShkSkqRSlZ8FVpK63bZdo2zevo+DR8ZYOjjAxrUrWLd6uNNldQVDQtKitm3XKJu27mHs2DgAo0fG2LR1D4BBgd1Nkha5zdv3nQyImrFj42zevq9DFXUXQ0LSonbwyNic2hcbQ0LSorZ0cGBO7YuNISFpUdu4dgUD/X2T2gb6+9i4dkWHKuoule+4jogngJ8B48DxzByJiCXAncA5wBPAOzLz/1VdiyRNVds57eimxiIzq32BiZAYycwf1bV9HHg6M2+KiGuBMzLzw9M9z8jISO7YsaPSWiVpoYmInZk50uz8nepuugK4rbh9G7CuQ3VIkqbRjpBI4EsRsTMiNhRtL8/MQwDF75c1mjEiNkTEjojYcfjw4TaUKkmq146D6dZk5sGIeBlwb0Q8MtsZM3MLsAUmupuqKlCS1FjlWxKZebD4/RTwBeBi4MmIOBOg+P1U1XVIkuau0pCIiNMj4oW128CvAHuBe4BrioddA3yxyjokSc2purvp5cAXIqL2Wp/NzL+JiG8Dd0XEe4AfAG+vuA5JUhMqDYnMfBz4xQbtPwYuqfK1JUnz5xHXkqRShoQkqZQhIUkqZUhIkkoZEpKkUoaEJKmUISFJKmVISJJKGRKSpFKGhCSplCEhSSplSEiSShkSkqRShoQkqZQhIUkqZUhIkkoZEpKkUoaEJKlUpSEREWdHxFcj4uGIeCgifqdovz4iRiNid/FzeZV1SJKaU+k1roHjwH/MzAci4oXAzoi4t5j2qcz8RMWvL0mah0pDIjMPAYeK2z+LiIeB4SpfU5LUOm3bJxER5wCrgW8WTR+IiAcj4taIOKNkng0RsSMidhw+fLhdpUqSCm0JiYh4AXA38MHM/CnwGeA8YBUTWxo3N5ovM7dk5khmjgwNDbWjVElSncpDIiL6mQiI2zNzK0BmPpmZ45l5ArgFuLjqOiRJc1f16KYA/hR4ODM/Wdd+Zt3DrgT2VlmHJKk5VY9uWgO8G9gTEbuLto8AV0fEKiCBJ4D3VlyHJKkJVY9u+j9ANJj011W+riSpNTziWpJUquruJmlBe9ct3+Dr+5+e9ePXnLeE23/7lyqsSGotQ0Jqwke37eF/3P+DOc/39f1P865bvnHydrNO6z+F/3LVL7ButcemqlqGhDRHzQZEzXzCoeaZYyf40F27AQwKVcp9EtIc3fHNH3a6BABOJGzevq/TZWiBc0uiS2zbNcrm7fs4eGSMpYMDbFy7wjXELjWe2ekSTjp4ZKzTJWiBMyS6wLZdo2zauoexY+MAjB4ZY9PWPYBdCd2oL6JrgmLp4EDbX9MVmsXF7qYusHn7vpMBUTN2bNyuhC519WvPbnreNectYc15S1pSxykBG9euaMlzzVZthWb0yBjJsys023aNtrUOtY9bEl2grMvAroTudMO6lQAz7rx+0al9PPifLms4ba5DZ6fq1Oim6VZo3JpYmAyJLrB0cIDRBoHQia4Ezc4N61aeDItm9OqxEgtthcaus5nZ3dQFNq5dwUB/36S2gf6+tnclSDMpW3HpxRUau85mx5DoAutWD3PjVSsZHhwggOHBAW68aqVrNOo6C2mFxn2Bs2N3U5dYt3p43qHgprOqVvs8LYTP2ULrOquKIbFAOIxW7dKKFZpu4L7A2TEkFghHnbglVYWFvEw3rl0xacUKJrrO3njBEGtu+sqC/JubYUgsEIt909ktqdbr9WVaFnD17S8e6Oef9J/CkWeOsXRwgDdeMMTdO0d79m+ugiGxQPTSpnMVa6et3JLatmuU3/38dzh+orqjqodb9HdXuabfy1unZQG34/tPTwqBI2PHGOjv41PvXMW61cOsuekrPfs3V8WQWCDKNp27bdRJVWunZVtMo0fGWH7tX/HigX4iOLnGWPbPdNuuUf7Dnbup+qQbrfi7W7UsP7ptD3d884eMZ9IXwdWvPZsb1q2sbOt0Nmv4tXaAj2x9kGeOnQAgAt712mWMvHLJpK2B+vf2jRcMnfx76o0dGy9tr4XAYt8ibySyQ+egiYjLgP8K9AF/kpk3Tff4kZGR3LFjR1tq61W90H+85qavNNziGR4c4OvXvmnaeaf+fW+8YIivPnK44fNpen0B4zN89QMahmX9e1UWMDXbdo1y/T0PcWTsGACnP7+Pnx8/wbG6rbQA/sV5S3jgBz+ZtJLT3xccKynyFODEbP7QWQrgezf96rw+n90qInZm5kiz83dkSyIi+oD/DrwZOAB8OyLuyczvdqKehaIXRp00u6bWaK15Ptd0WOxmCghoHBD1W6dTr6sxnnny/g3rVrJt1ygbP/+dSYHwjz+f3JVTe51GpygpCwhoPiDKTs5Y65btlS3ydurUwXQXA49l5uOZ+XPgc8AVHapFbdTsEbuN+sfVXn0Rkw7yLLuuRq198/Z9kwKi0wb6+7j6tWdPezCgB7Y+V6f2SQwD9Z+wA8Brpz4oIjYAGwCWLVvWnspUqWbX1BZzn3C3OJE56Z9l2enSa+3d9p79+kXD3LBu5aT9GY26ZXthi7ydOhUS0aDtOZ+4zNwCbIGJfRJVF6XqNXvEbtnoLbXP1K29sq6bvoiTj5/LezZ1H0ir90ncvXOUkVcuMQTmqFPdTQeA+pPynwUc7FAtarN1q4f5+rVv4ns3/Spfv/ZNs/rCNjpnkNqn0dZe2XU1au0b166g/5Tnrg82WkMc6O/jXa9bNqmbZ/O//kU+/c5VnNb/7L+pCPiN1y3jk+9cdfKxgwP9nHFaf8Pnred5mZrTqS2JbwPnR8RyYBRYD/ybDtWiHtBoC+Sclwzwf/c/Xflw1YVoNqObasqO6aiNYiob3VR7fP3opjNO6+f333ohMPutybm0l41Oqum2LrBe0MkhsJcDn2ZiCOytmfmx6R7vEFg1UhsW20xX1OBAP9e/7UI+eOfu1hc2jTXnLWH50AumHTrablNHj8HE2n2v7bRt9HfU6+WhrM2a7xDYjoXEXBkSmslHt+3h9vt/8Jx+7dOf/zx+MlZ+EN2bP/m3PPrUP7alxjXnLenaCw71wnE2szH12IyaXgy9VjAkpDrN/qMrC4pWnT5D7bdQQm++DAlJUqmePOJamq3p1gZdU5SqZ0ioa013Ajugp09jLfUKQ0Jda6ZrEHtKZ6l6hoS6VjMnA3QcvNRanTriWprRdCcDbPZEgZLmxi0Jda2ZTgY4mxMFvuuWbzQ8DfVUw3XXp3BHuPQsh8CqqzUzumnqdQ7a6dPFZTClbuFxElKdTgZEjUGhbjLfkHCfhBaUsgvhtJNnGtVC4j4JLShlF8JpJ0dYdZ4HWraOIaEFpexCOO3kCKvOmu4gTINi7uxu0oJSdiGcmQwPDvAbr1vGQP/8vxIzXYpV1ZrpIEzNjVsSWlBq12SYzc7rRqftvmHdynnt/J7LTmu7RKrRzEGYKufoJqkDFspFfrpR2dXpFuMFh8DRTVJPskukOo2uh97oQEvNjt1NC5RdGd3NLpHqNLoeup//5lUWEhGxGXgr8HNgP/CbmXkkIs4BHgZqq0z3Z+b7qqpjMXJ0R/dbOjjQsEvEkVGtsW71sJ/1Fqmyu+le4J9n5i8Afw9sqpu2PzNXFT8GRIvZldH97BJRr6gsJDLzS5l5vLh7P3BWVa+lyezK6H7rVg9z41UrGR4cIJjYqepOa3Wjdu2T+C3gzrr7yyNiF/BT4KOZ+b8bzRQRG4ANAMuWLau8yIXCrozeYJeIesG8tiQi4r6I2Nvg54q6x1wHHAduL5oOAcsyczXwIeCzEfGiRs+fmVsycyQzR4aGhuZT6qJiV4akVpnXlkRmXjrd9Ii4Bvg14JIsDsjIzKPA0eL2zojYD7wK8CCIFnF0h6RWqXJ002XAh4F/lZnP1LUPAU9n5nhEnAucDzxeVR2LlV0Zklqhyn0SfwicCtwbEfDsUNc3AP85Io4D48D7MnPmS4dJktquspDIzH9a0n43cHdVrytJah1PyyFJKmVISJJKGRKSpFKGhCSplCEhSSplSEiSShkSkqRShoQkqZQhIUkqZUhIkkoZEpKkUoaEJKlUu65MJ2mWtu0a9Vog6hqGhNRFtu0aZdPWPYwdGwdg9MgYm7buATAo1BF2N0ldZPP2fScDombs2Dibt+/rUEVa7AwJqYscPDI2p3apaoaE1EWWDg7MqV2qmiEhdZGNa1cw0N83qW2gv4+Na1d0qCItdpWFRERcHxGjEbG7+Lm8btqmiHgsIvZFxNqqapB6zbrVw9x41UqGBwcIYHhwgBuvWulOa3VM1aObPpWZn6hviIhXA+uBC4GlwH0R8arMHG/0BNJis271sKGgrtGJ7qYrgM9l5tHM/B7wGHBxB+qQJM2g6pD4QEQ8GBG3RsQZRdsw8MO6xxwo2p4jIjZExI6I2HH48OGKS5UkTTWvkIiI+yJib4OfK4DPAOcBq4BDwM212Ro8VTZ6/szckpkjmTkyNDQ0n1IlSU2Y1z6JzLx0No+LiFuAvyzuHgDOrpt8FnBwPnVIkqpR5eimM+vuXgnsLW7fA6yPiFMjYjlwPvCtquqQJDWvytFNH4+IVUx0JT0BvBcgMx+KiLuA7wLHgfc7skmSulNlIZGZ755m2seAj1X12pKk1uiZI64f+YefsW3XaKfLkKRFpWdC4tj4CTZt3WNQSFIb9UxIgKdMlqR266mQAE+ZLEnt1HMh4SmTJal9eiokPGWyJLVXz4REf98pnjJZktqsZ0Ligle80ICQpDbrmZCQJLWfISFJKmVISJJKGRKSpFKGhCSplCEhSSplSEiSShkSkqRShoQkqZQhIUkqVdnlSyPiTqB2Nr5B4EhmroqIc4CHgdqFIe7PzPdVVYckqXlVXuP6nbXbEXEz8JO6yfszc1VVry1Jao3KQqImIgJ4B/Cmql9LktRa7dgn8Xrgycx8tK5teUTsioi/i4jXt6EGSVIT5rUlERH3Aa9oMOm6zPxicftq4I66aYeAZZn544i4CNgWERdm5k8bPP8GYAPAsmXL5lOqJKkJ8wqJzLx0uukR8TzgKuCiunmOAkeL2zsjYj/wKmBHg+ffAmwBGBkZyfnUKkm9aNuuUTZv38fBI2MsHRxg49oVbb22TtX7JC4FHsnMA7WGiBgCns7M8Yg4FzgfeLziOiSp52zbNcqmrXsYOzYOwOiRMTZt3QPQtqCoep/EeiZ3NQG8AXgwIr4D/E/gfZn5dMV1SFLP2bx938mAqBk7Ns7m7ftK5mi9SrckMvPfNWi7G7i7yteVpIXg4JGxObVXwSOuJalLLR0cmFN7FQwJSepSG9euYKC/b1LbQH8fG9euKJmj9So/mE6S1JzazumFPLpJkjQP61YPtzUUprK7SZJUypCQJJUyJCRJpQwJSVIpQ0KSVMqQkCSVMiQkSaUMCUlSKUNCklTKkJAklTIkJEmlDAlJUilDQpJUypCQJJUyJCRJpeYVEhHx9oh4KCJORMTIlGmbIuKxiNgXEWvr2i+KiD3FtP8WETGfGiRJ1ZnvlsRe4Crga/WNEfFqYD1wIXAZ8EcRUbsG32eADcD5xc9l86xBklSReYVEZj6cmfsaTLoC+FxmHs3M7wGPARdHxJnAizLzG5mZwJ8D6+ZTgySpOlVdvnQYuL/u/oGi7Vhxe2p7QxGxgYmtDoCjEbG3xXVW4aXAjzpdxAx6oUawzlazztbqlTpXzGfmGUMiIu4DXtFg0nWZ+cWy2Rq05TTtDWXmFmBLUceOzBwpe2y36IU6e6FGsM5Ws87W6qU65zP/jCGRmZc28bwHgLPr7p8FHCzaz2rQLknqQlUNgb0HWB8Rp0bEciZ2UH8rMw8BP4uI1xWjmv4tULY1IknqsPkOgb0yIg4AvwT8VURsB8jMh4C7gO8CfwO8PzPHi9n+PfAnTOzM3g/8r1m+3Jb51NpGvVBnL9QI1tlq1tlai6LOmBhkJEnSc3nEtSSplCEhSSrVdSHRi6f6iIg7I2J38fNEROwu2s+JiLG6aX/czroa1Hl9RIzW1XN53bSGy7ZDdW6OiEci4sGI+EJEDBbtXbU8i5ouK5bZYxFxbafrAYiIsyPiqxHxcPFd+p2ivfT972CtTxTf3d21oZoRsSQi7o2IR4vfZ3S4xhV1y2x3RPw0Ij7YDcszIm6NiKfqjyGbbvk19T3PzK76Af4ZEwd//C0wUtf+auA7wKnAciZ2evcV077FxM7zYGJH+Fs6WP/NwO8Vt88B9nZ6mdbVdj3wuw3aS5dth+r8FeB5xe0/AP6gS5dnX7GszgWeXyzDV3dBXWcCryluvxD4++I9bvj+d7jWJ4CXTmn7OHBtcfva2vvfDT/Fe/4PwCu7YXkCbwBeU/+9KFt+zX7Pu25LInv4VB/FFsw7gDs68frz0HDZdqqYzPxSZh4v7t7P5GNrusnFwGOZ+Xhm/hz4HBPLsqMy81BmPlDc/hnwMNOc2aALXQHcVty+je46dc8lwP7M/H6nCwHIzK8BT09pLlt+TX3Puy4kpjEM/LDufu2UHsPM4VQfFXs98GRmPlrXtjwidkXE30XE6ztUV70PFN04t9ZthpYt227wW0weJt1Ny7Oblxsw0UUHrAa+WTQ1ev87KYEvRcTOmDgND8DLc+KYKorfL+tYdc+1nskrgd22PKF8+TX1ee1ISETEfRGxt8HPdGthLTnVR7NmWfPVTP4AHQKWZeZq4EPAZyPiRa2ubQ51fgY4D1hV1HZzbbYGT1Xp2OjZLM+IuA44DtxeNLV9ec6g7cttLiLiBcDdwAcz86eUv/+dtCYzXwO8BXh/RLyh0wWViYjnA28DPl80dePynE5Tn9eqTvA3rezBU33MVHNEPI+J06ZfVDfPUeBocXtnROwHXgXM61wq86mzJiJuAf6yuFu2bCszi+V5DfBrwCVFN2JHlucM2r7cZisi+pkIiNszcytAZj5ZN73+/e+YzDxY/H4qIr7ARPfHkxFxZmYeKrqTn+pokc96C/BAbTl24/IslC2/pj6vvdTd1O2n+rgUeCQzT3Z9RcRQFNfRiIhzi5of70BttXrOrLt7JRPXA4GSZdvu+moi4jLgw8DbMvOZuvauWp7At4HzI2J5sZa5noll2VHF9+BPgYcz85N17WXvf0dExOkR8cLabSYGLOxlYhleUzzsGrrn1D2Tegq6bXnWKVt+zX3POz1aoMHe+iuZSLyjwJPA9rpp1zGxR34fdSOYgBEm3qD9wB9SHEne5rr/DHjflLZfBx5iYkTBA8BbO7xs/wLYAzxYfGDOnGnZdqjOx5joO91d/PxxNy7PoqbLmRg9tJ+JMyN3tJ6ipn/JRDfCg3XL8PLp3v8O1Xlu8V5+p3hfryvaXwJ8GXi0+L2kC5bpacCPgRfXtXV8eTIRWod49jIM75lu+TXzPfe0HJKkUr3U3SRJajNDQpJUypCQJJUyJCRJpQwJSVIpQ0KSVMqQkCSV+v/7+SUVkBQFZgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Ypred=Rede.predict(Xtest)\n",
    "plt.scatter(Ypred,Ytest)\n",
    "plt.xlim([-100,100])\n",
    "plt.ylim([-100,100])\n",
    "#plt.scatter(Xtest,Ypred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'finalized_model_sem_espaco.sav'\n",
    "pickle.dump(Rede, open(filename, 'wb'))\n",
    " \n",
    "# some time later...\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6465708993511641\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(Xtest, Ytest)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f2f6c680cf484319d387fabac80ca4ff4fc33965036e02100b3fe02600f1423"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
