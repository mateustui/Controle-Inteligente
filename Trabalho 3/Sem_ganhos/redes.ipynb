{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Activation\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dados = pd.read_csv(r'C:\\Users\\Mateus\\Meu Drive\\Compartilhado\\eng\\9_periodo\\Controle inteligente\\Trabalho 3\\Sem_ganhos\\teste4.csv', on_bad_lines='skip', header=None)\n",
    "# Dados.values\n",
    "# Dados.head(5)\n",
    "# print(Dados)\n",
    "\n",
    "Entradas = Dados.iloc[:,:-1]\n",
    "Entradas.shape\n",
    "Saidas = Dados.iloc[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.012617</td>\n",
       "      <td>-11.945530</td>\n",
       "      <td>0.025040</td>\n",
       "      <td>17.970368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.132073</td>\n",
       "      <td>0.133452</td>\n",
       "      <td>0.204744</td>\n",
       "      <td>-0.138744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.130738</td>\n",
       "      <td>1.976486</td>\n",
       "      <td>0.203356</td>\n",
       "      <td>-2.815665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.110973</td>\n",
       "      <td>1.390203</td>\n",
       "      <td>0.175200</td>\n",
       "      <td>-1.924673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.097071</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>0.155953</td>\n",
       "      <td>0.153231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83371</th>\n",
       "      <td>0.000160</td>\n",
       "      <td>-0.000197</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>-0.000033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83372</th>\n",
       "      <td>0.000158</td>\n",
       "      <td>-0.000194</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>-0.000032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83373</th>\n",
       "      <td>0.000156</td>\n",
       "      <td>-0.000192</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>-0.000032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83374</th>\n",
       "      <td>0.000154</td>\n",
       "      <td>-0.000189</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>-0.000032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83375</th>\n",
       "      <td>0.000152</td>\n",
       "      <td>-0.000187</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>-0.000031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83376 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1         2          3\n",
       "0     -1.012617 -11.945530  0.025040  17.970368\n",
       "1     -1.132073   0.133452  0.204744  -0.138744\n",
       "2     -1.130738   1.976486  0.203356  -2.815665\n",
       "3     -1.110973   1.390203  0.175200  -1.924673\n",
       "4     -1.097071   0.000745  0.155953   0.153231\n",
       "...         ...        ...       ...        ...\n",
       "83371  0.000160  -0.000197  0.000027  -0.000033\n",
       "83372  0.000158  -0.000194  0.000027  -0.000032\n",
       "83373  0.000156  -0.000192  0.000027  -0.000032\n",
       "83374  0.000154  -0.000189  0.000026  -0.000032\n",
       "83375  0.000152  -0.000187  0.000026  -0.000031\n",
       "\n",
       "[83376 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Entradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest=train_test_split(Entradas,Saidas,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2680.59246802\n",
      "Iteration 2, loss = 2634.27198273\n",
      "Iteration 3, loss = 2406.20402656\n",
      "Iteration 4, loss = 2108.08782302\n",
      "Iteration 5, loss = 2025.08691630\n",
      "Iteration 6, loss = 2006.44035201\n",
      "Iteration 7, loss = 1994.04322338\n",
      "Iteration 8, loss = 1983.46968536\n",
      "Iteration 9, loss = 1974.40194163\n",
      "Iteration 10, loss = 1967.10525410\n",
      "Iteration 11, loss = 1961.05907409\n",
      "Iteration 12, loss = 1956.73546063\n",
      "Iteration 13, loss = 1952.62975645\n",
      "Iteration 14, loss = 1949.35032428\n",
      "Iteration 15, loss = 1945.55098658\n",
      "Iteration 16, loss = 1942.83024502\n",
      "Iteration 17, loss = 1939.89561272\n",
      "Iteration 18, loss = 1938.29202370\n",
      "Iteration 19, loss = 1935.83505027\n",
      "Iteration 20, loss = 1933.71573270\n",
      "Iteration 21, loss = 1931.72264979\n",
      "Iteration 22, loss = 1929.52578302\n",
      "Iteration 23, loss = 1928.39807753\n",
      "Iteration 24, loss = 1925.92368211\n",
      "Iteration 25, loss = 1924.66718302\n",
      "Iteration 26, loss = 1923.77068650\n",
      "Iteration 27, loss = 1921.46775305\n",
      "Iteration 28, loss = 1919.34320263\n",
      "Iteration 29, loss = 1917.96847736\n",
      "Iteration 30, loss = 1916.45220409\n",
      "Iteration 31, loss = 1914.51097321\n",
      "Iteration 32, loss = 1912.92792723\n",
      "Iteration 33, loss = 1912.09893073\n",
      "Iteration 34, loss = 1910.19074844\n",
      "Iteration 35, loss = 1908.90024356\n",
      "Iteration 36, loss = 1907.85156802\n",
      "Iteration 37, loss = 1906.48443742\n",
      "Iteration 38, loss = 1905.43428356\n",
      "Iteration 39, loss = 1903.73703736\n",
      "Iteration 40, loss = 1902.47962949\n",
      "Iteration 41, loss = 1901.34542860\n",
      "Iteration 42, loss = 1899.38535073\n",
      "Iteration 43, loss = 1898.37845851\n",
      "Iteration 44, loss = 1896.83115620\n",
      "Iteration 45, loss = 1895.57535860\n",
      "Iteration 46, loss = 1894.15950100\n",
      "Iteration 47, loss = 1892.92837989\n",
      "Iteration 48, loss = 1891.37418751\n",
      "Iteration 49, loss = 1890.90739675\n",
      "Iteration 50, loss = 1889.73354697\n",
      "Iteration 51, loss = 1887.62506492\n",
      "Iteration 52, loss = 1887.00119185\n",
      "Iteration 53, loss = 1885.13756958\n",
      "Iteration 54, loss = 1883.60038933\n",
      "Iteration 55, loss = 1882.57945479\n",
      "Iteration 56, loss = 1880.50218797\n",
      "Iteration 57, loss = 1878.36030789\n",
      "Iteration 58, loss = 1875.25228588\n",
      "Iteration 59, loss = 1871.35002633\n",
      "Iteration 60, loss = 1867.10066952\n",
      "Iteration 61, loss = 1863.78675838\n",
      "Iteration 62, loss = 1861.37288216\n",
      "Iteration 63, loss = 1859.12050822\n",
      "Iteration 64, loss = 1855.71238879\n",
      "Iteration 65, loss = 1853.72742035\n",
      "Iteration 66, loss = 1850.87268972\n",
      "Iteration 67, loss = 1848.76240707\n",
      "Iteration 68, loss = 1846.80984521\n",
      "Iteration 69, loss = 1845.22087046\n",
      "Iteration 70, loss = 1841.70717885\n",
      "Iteration 71, loss = 1840.36837546\n",
      "Iteration 72, loss = 1837.02394371\n",
      "Iteration 73, loss = 1834.82760586\n",
      "Iteration 74, loss = 1833.20909144\n",
      "Iteration 75, loss = 1830.31209751\n",
      "Iteration 76, loss = 1828.47945985\n",
      "Iteration 77, loss = 1825.64339120\n",
      "Iteration 78, loss = 1823.92438973\n",
      "Iteration 79, loss = 1821.60727511\n",
      "Iteration 80, loss = 1818.47125567\n",
      "Iteration 81, loss = 1816.54557431\n",
      "Iteration 82, loss = 1813.71983927\n",
      "Iteration 83, loss = 1811.59024908\n",
      "Iteration 84, loss = 1808.77254511\n",
      "Iteration 85, loss = 1806.13932246\n",
      "Iteration 86, loss = 1803.33787134\n",
      "Iteration 87, loss = 1801.03767339\n",
      "Iteration 88, loss = 1798.65090099\n",
      "Iteration 89, loss = 1796.29818718\n",
      "Iteration 90, loss = 1793.32342374\n",
      "Iteration 91, loss = 1791.16791807\n",
      "Iteration 92, loss = 1787.26409691\n",
      "Iteration 93, loss = 1786.92717046\n",
      "Iteration 94, loss = 1783.90386085\n",
      "Iteration 95, loss = 1781.25499279\n",
      "Iteration 96, loss = 1778.96695861\n",
      "Iteration 97, loss = 1776.96954552\n",
      "Iteration 98, loss = 1773.16096552\n",
      "Iteration 99, loss = 1770.98233733\n",
      "Iteration 100, loss = 1767.44174437\n",
      "Iteration 101, loss = 1765.90422595\n",
      "Iteration 102, loss = 1762.89349602\n",
      "Iteration 103, loss = 1760.96776063\n",
      "Iteration 104, loss = 1758.14901907\n",
      "Iteration 105, loss = 1754.59473141\n",
      "Iteration 106, loss = 1752.59662557\n",
      "Iteration 107, loss = 1750.57599257\n",
      "Iteration 108, loss = 1748.22090896\n",
      "Iteration 109, loss = 1745.22977181\n",
      "Iteration 110, loss = 1742.76749475\n",
      "Iteration 111, loss = 1739.17076551\n",
      "Iteration 112, loss = 1737.34819175\n",
      "Iteration 113, loss = 1734.19349864\n",
      "Iteration 114, loss = 1730.68194363\n",
      "Iteration 115, loss = 1729.42046604\n",
      "Iteration 116, loss = 1725.84936479\n",
      "Iteration 117, loss = 1724.26721169\n",
      "Iteration 118, loss = 1722.79369268\n",
      "Iteration 119, loss = 1718.85560067\n",
      "Iteration 120, loss = 1716.26833468\n",
      "Iteration 121, loss = 1713.74008389\n",
      "Iteration 122, loss = 1710.85305897\n",
      "Iteration 123, loss = 1708.59660422\n",
      "Iteration 124, loss = 1705.78032870\n",
      "Iteration 125, loss = 1701.83739507\n",
      "Iteration 126, loss = 1699.35952772\n",
      "Iteration 127, loss = 1699.06975039\n",
      "Iteration 128, loss = 1693.92493281\n",
      "Iteration 129, loss = 1691.46310766\n",
      "Iteration 130, loss = 1690.54697621\n",
      "Iteration 131, loss = 1687.07946937\n",
      "Iteration 132, loss = 1684.86572318\n",
      "Iteration 133, loss = 1682.68272049\n",
      "Iteration 134, loss = 1677.36837334\n",
      "Iteration 135, loss = 1676.86462318\n",
      "Iteration 136, loss = 1674.65624029\n",
      "Iteration 137, loss = 1672.26521306\n",
      "Iteration 138, loss = 1668.72298150\n",
      "Iteration 139, loss = 1667.72342249\n",
      "Iteration 140, loss = 1664.41593269\n",
      "Iteration 141, loss = 1661.37736914\n",
      "Iteration 142, loss = 1659.43629800\n",
      "Iteration 143, loss = 1655.56824166\n",
      "Iteration 144, loss = 1651.19227796\n",
      "Iteration 145, loss = 1652.71323048\n",
      "Iteration 146, loss = 1649.84923962\n",
      "Iteration 147, loss = 1646.86477354\n",
      "Iteration 148, loss = 1646.64512752\n",
      "Iteration 149, loss = 1641.75903937\n",
      "Iteration 150, loss = 1640.28396737\n",
      "Iteration 151, loss = 1639.24657075\n",
      "Iteration 152, loss = 1637.06522689\n",
      "Iteration 153, loss = 1636.59063254\n",
      "Iteration 154, loss = 1635.41139166\n",
      "Iteration 155, loss = 1631.23075185\n",
      "Iteration 156, loss = 1632.62136015\n",
      "Iteration 157, loss = 1629.73432129\n",
      "Iteration 158, loss = 1629.52259319\n",
      "Iteration 159, loss = 1628.74556350\n",
      "Iteration 160, loss = 1628.29389639\n",
      "Iteration 161, loss = 1626.36973995\n",
      "Iteration 162, loss = 1625.52118873\n",
      "Iteration 163, loss = 1623.77642942\n",
      "Iteration 164, loss = 1623.41402022\n",
      "Iteration 165, loss = 1624.09128796\n",
      "Iteration 166, loss = 1621.07571473\n",
      "Iteration 167, loss = 1617.50719685\n",
      "Iteration 168, loss = 1617.63371348\n",
      "Iteration 169, loss = 1616.55870529\n",
      "Iteration 170, loss = 1616.39483563\n",
      "Iteration 171, loss = 1613.87128044\n",
      "Iteration 172, loss = 1614.41024167\n",
      "Iteration 173, loss = 1613.26745582\n",
      "Iteration 174, loss = 1611.20122518\n",
      "Iteration 175, loss = 1610.01855871\n",
      "Iteration 176, loss = 1608.92316754\n",
      "Iteration 177, loss = 1605.99690603\n",
      "Iteration 178, loss = 1607.48509339\n",
      "Iteration 179, loss = 1605.23511018\n",
      "Iteration 180, loss = 1604.55808614\n",
      "Iteration 181, loss = 1602.80680205\n",
      "Iteration 182, loss = 1601.49353646\n",
      "Iteration 183, loss = 1601.89989547\n",
      "Iteration 184, loss = 1601.41005025\n",
      "Iteration 185, loss = 1599.02615433\n",
      "Iteration 186, loss = 1596.16047363\n",
      "Iteration 187, loss = 1598.40219999\n",
      "Iteration 188, loss = 1596.77632335\n",
      "Iteration 189, loss = 1594.72856638\n",
      "Iteration 190, loss = 1592.45696535\n",
      "Iteration 191, loss = 1593.83366346\n",
      "Iteration 192, loss = 1589.12194360\n",
      "Iteration 193, loss = 1591.03246131\n",
      "Iteration 194, loss = 1589.86161460\n",
      "Iteration 195, loss = 1589.39067741\n",
      "Iteration 196, loss = 1587.65987401\n",
      "Iteration 197, loss = 1588.48376468\n",
      "Iteration 198, loss = 1586.15607703\n",
      "Iteration 199, loss = 1584.78237064\n",
      "Iteration 200, loss = 1583.12226070\n",
      "Iteration 201, loss = 1585.45615473\n",
      "Iteration 202, loss = 1580.42788470\n",
      "Iteration 203, loss = 1580.51600967\n",
      "Iteration 204, loss = 1582.09650234\n",
      "Iteration 205, loss = 1579.92970070\n",
      "Iteration 206, loss = 1577.65824720\n",
      "Iteration 207, loss = 1577.70005122\n",
      "Iteration 208, loss = 1578.30134413\n",
      "Iteration 209, loss = 1575.29969010\n",
      "Iteration 210, loss = 1574.65356904\n",
      "Iteration 211, loss = 1574.66376484\n",
      "Iteration 212, loss = 1575.03021947\n",
      "Iteration 213, loss = 1570.00231114\n",
      "Iteration 214, loss = 1571.69377703\n",
      "Iteration 215, loss = 1569.05190436\n",
      "Iteration 216, loss = 1569.05170403\n",
      "Iteration 217, loss = 1568.32191285\n",
      "Iteration 218, loss = 1566.99045126\n",
      "Iteration 219, loss = 1568.14882616\n",
      "Iteration 220, loss = 1565.04531682\n",
      "Iteration 221, loss = 1564.89253136\n",
      "Iteration 222, loss = 1563.37692541\n",
      "Iteration 223, loss = 1564.67124736\n",
      "Iteration 224, loss = 1561.20018512\n",
      "Iteration 225, loss = 1559.92683999\n",
      "Iteration 226, loss = 1561.57613148\n",
      "Iteration 227, loss = 1558.61645293\n",
      "Iteration 228, loss = 1559.30939361\n",
      "Iteration 229, loss = 1555.86337333\n",
      "Iteration 230, loss = 1557.74808682\n",
      "Iteration 231, loss = 1556.76535589\n",
      "Iteration 232, loss = 1556.59755722\n",
      "Iteration 233, loss = 1554.26192032\n",
      "Iteration 234, loss = 1552.03623087\n",
      "Iteration 235, loss = 1552.77539033\n",
      "Iteration 236, loss = 1549.33904263\n",
      "Iteration 237, loss = 1552.83385273\n",
      "Iteration 238, loss = 1549.82253231\n",
      "Iteration 239, loss = 1549.26346921\n",
      "Iteration 240, loss = 1549.00323982\n",
      "Iteration 241, loss = 1546.69897345\n",
      "Iteration 242, loss = 1546.13424298\n",
      "Iteration 243, loss = 1549.97948436\n",
      "Iteration 244, loss = 1546.33978569\n",
      "Iteration 245, loss = 1545.16346452\n",
      "Iteration 246, loss = 1544.51992647\n",
      "Iteration 247, loss = 1544.89266808\n",
      "Iteration 248, loss = 1543.03486705\n",
      "Iteration 249, loss = 1543.14322842\n",
      "Iteration 250, loss = 1540.94903634\n",
      "Iteration 251, loss = 1535.95365969\n",
      "Iteration 252, loss = 1537.63821827\n",
      "Iteration 253, loss = 1537.35344505\n",
      "Iteration 254, loss = 1538.71802718\n",
      "Iteration 255, loss = 1539.28338258\n",
      "Iteration 256, loss = 1535.73160932\n",
      "Iteration 257, loss = 1534.56967664\n",
      "Iteration 258, loss = 1534.08164770\n",
      "Iteration 259, loss = 1533.37916221\n",
      "Iteration 260, loss = 1532.16380302\n",
      "Iteration 261, loss = 1532.65799181\n",
      "Iteration 262, loss = 1530.67992327\n",
      "Iteration 263, loss = 1533.01180051\n",
      "Iteration 264, loss = 1527.82917383\n",
      "Iteration 265, loss = 1531.79933578\n",
      "Iteration 266, loss = 1527.55873503\n",
      "Iteration 267, loss = 1529.70798781\n",
      "Iteration 268, loss = 1528.57531922\n",
      "Iteration 269, loss = 1527.36462452\n",
      "Iteration 270, loss = 1525.01547148\n",
      "Iteration 271, loss = 1525.36101531\n",
      "Iteration 272, loss = 1524.58415569\n",
      "Iteration 273, loss = 1524.33532432\n",
      "Iteration 274, loss = 1522.48833426\n",
      "Iteration 275, loss = 1523.77230441\n",
      "Iteration 276, loss = 1522.85182505\n",
      "Iteration 277, loss = 1522.54930066\n",
      "Iteration 278, loss = 1520.57910609\n",
      "Iteration 279, loss = 1519.92757429\n",
      "Iteration 280, loss = 1520.77707193\n",
      "Iteration 281, loss = 1517.82979001\n",
      "Iteration 282, loss = 1515.32214307\n",
      "Iteration 283, loss = 1518.83603360\n",
      "Iteration 284, loss = 1517.41250156\n",
      "Iteration 285, loss = 1515.75511361\n",
      "Iteration 286, loss = 1514.62166242\n",
      "Iteration 287, loss = 1514.55900811\n",
      "Iteration 288, loss = 1515.49520201\n",
      "Iteration 289, loss = 1514.98796285\n",
      "Iteration 290, loss = 1513.48524696\n",
      "Iteration 291, loss = 1513.11717684\n",
      "Iteration 292, loss = 1510.07772425\n",
      "Iteration 293, loss = 1513.36795197\n",
      "Iteration 294, loss = 1512.57478164\n",
      "Iteration 295, loss = 1510.03587752\n",
      "Iteration 296, loss = 1509.61635504\n",
      "Iteration 297, loss = 1508.49899515\n",
      "Iteration 298, loss = 1507.51122783\n",
      "Iteration 299, loss = 1506.10025238\n",
      "Iteration 300, loss = 1510.61418011\n",
      "Iteration 301, loss = 1507.78305463\n",
      "Iteration 302, loss = 1502.76581020\n",
      "Iteration 303, loss = 1506.57269640\n",
      "Iteration 304, loss = 1502.43329569\n",
      "Iteration 305, loss = 1502.89790774\n",
      "Iteration 306, loss = 1502.30869965\n",
      "Iteration 307, loss = 1498.59132101\n",
      "Iteration 308, loss = 1500.89072384\n",
      "Iteration 309, loss = 1500.51609343\n",
      "Iteration 310, loss = 1500.14954173\n",
      "Iteration 311, loss = 1499.23218817\n",
      "Iteration 312, loss = 1499.26419547\n",
      "Iteration 313, loss = 1499.63026757\n",
      "Iteration 314, loss = 1496.51157156\n",
      "Iteration 315, loss = 1496.81080852\n",
      "Iteration 316, loss = 1494.78987295\n",
      "Iteration 317, loss = 1497.22126619\n",
      "Iteration 318, loss = 1495.62442099\n",
      "Iteration 319, loss = 1493.63139186\n",
      "Iteration 320, loss = 1492.76209984\n",
      "Iteration 321, loss = 1493.22395739\n",
      "Iteration 322, loss = 1488.04430501\n",
      "Iteration 323, loss = 1492.18989797\n",
      "Iteration 324, loss = 1490.26984368\n",
      "Iteration 325, loss = 1491.04461064\n",
      "Iteration 326, loss = 1490.01052242\n",
      "Iteration 327, loss = 1487.24495633\n",
      "Iteration 328, loss = 1487.37958683\n",
      "Iteration 329, loss = 1488.76801704\n",
      "Iteration 330, loss = 1488.91142336\n",
      "Iteration 331, loss = 1486.99738023\n",
      "Iteration 332, loss = 1485.58840367\n",
      "Iteration 333, loss = 1485.25424851\n",
      "Iteration 334, loss = 1485.40399229\n",
      "Iteration 335, loss = 1482.95730115\n",
      "Iteration 336, loss = 1485.06714201\n",
      "Iteration 337, loss = 1483.14818979\n",
      "Iteration 338, loss = 1483.37947329\n",
      "Iteration 339, loss = 1481.48140775\n",
      "Iteration 340, loss = 1482.75138459\n",
      "Iteration 341, loss = 1479.79876416\n",
      "Iteration 342, loss = 1480.63921067\n",
      "Iteration 343, loss = 1479.56157232\n",
      "Iteration 344, loss = 1479.34101211\n",
      "Iteration 345, loss = 1477.22319996\n",
      "Iteration 346, loss = 1478.52998067\n",
      "Iteration 347, loss = 1477.38528463\n",
      "Iteration 348, loss = 1475.11933680\n",
      "Iteration 349, loss = 1476.51218901\n",
      "Iteration 350, loss = 1476.81126070\n",
      "Iteration 351, loss = 1475.72394241\n",
      "Iteration 352, loss = 1473.78543799\n",
      "Iteration 353, loss = 1472.13331907\n",
      "Iteration 354, loss = 1471.71467823\n",
      "Iteration 355, loss = 1472.37416247\n",
      "Iteration 356, loss = 1471.38451946\n",
      "Iteration 357, loss = 1470.63585917\n",
      "Iteration 358, loss = 1470.33577959\n",
      "Iteration 359, loss = 1466.38014988\n",
      "Iteration 360, loss = 1467.36359923\n",
      "Iteration 361, loss = 1469.34344356\n",
      "Iteration 362, loss = 1467.83647165\n",
      "Iteration 363, loss = 1467.78537868\n",
      "Iteration 364, loss = 1466.56980372\n",
      "Iteration 365, loss = 1464.97213718\n",
      "Iteration 366, loss = 1466.42723708\n",
      "Iteration 367, loss = 1466.08712506\n",
      "Iteration 368, loss = 1463.94767217\n",
      "Iteration 369, loss = 1463.46322993\n",
      "Iteration 370, loss = 1462.90512534\n",
      "Iteration 371, loss = 1463.61655045\n",
      "Iteration 372, loss = 1462.44541578\n",
      "Iteration 373, loss = 1462.02821866\n",
      "Iteration 374, loss = 1459.39457037\n",
      "Iteration 375, loss = 1460.37889039\n",
      "Iteration 376, loss = 1458.15547134\n",
      "Iteration 377, loss = 1459.27757180\n",
      "Iteration 378, loss = 1459.41876845\n",
      "Iteration 379, loss = 1460.23332940\n",
      "Iteration 380, loss = 1457.18855844\n",
      "Iteration 381, loss = 1457.45292342\n",
      "Iteration 382, loss = 1455.26327099\n",
      "Iteration 383, loss = 1458.37404083\n",
      "Iteration 384, loss = 1456.18473002\n",
      "Iteration 385, loss = 1453.45931917\n",
      "Iteration 386, loss = 1454.75959114\n",
      "Iteration 387, loss = 1450.76847141\n",
      "Iteration 388, loss = 1454.56026478\n",
      "Iteration 389, loss = 1452.26745335\n",
      "Iteration 390, loss = 1453.54415196\n",
      "Iteration 391, loss = 1452.71986133\n",
      "Iteration 392, loss = 1451.12908539\n",
      "Iteration 393, loss = 1449.74530548\n",
      "Iteration 394, loss = 1450.72099584\n",
      "Iteration 395, loss = 1447.10924226\n",
      "Iteration 396, loss = 1451.30646956\n",
      "Iteration 397, loss = 1446.69472272\n",
      "Iteration 398, loss = 1448.12446114\n",
      "Iteration 399, loss = 1448.36849073\n",
      "Iteration 400, loss = 1447.22502322\n",
      "Iteration 401, loss = 1444.50805720\n",
      "Iteration 402, loss = 1446.53500526\n",
      "Iteration 403, loss = 1445.76474161\n",
      "Iteration 404, loss = 1446.71161469\n",
      "Iteration 405, loss = 1442.92918491\n",
      "Iteration 406, loss = 1444.26121260\n",
      "Iteration 407, loss = 1443.34190218\n",
      "Iteration 408, loss = 1442.43246431\n",
      "Iteration 409, loss = 1438.89138146\n",
      "Iteration 410, loss = 1443.38008337\n",
      "Iteration 411, loss = 1442.02147017\n",
      "Iteration 412, loss = 1440.07402033\n",
      "Iteration 413, loss = 1442.36897650\n",
      "Iteration 414, loss = 1441.93060040\n",
      "Iteration 415, loss = 1437.91246503\n",
      "Iteration 416, loss = 1440.59580015\n",
      "Iteration 417, loss = 1436.10958834\n",
      "Iteration 418, loss = 1437.15051154\n",
      "Iteration 419, loss = 1434.85365174\n",
      "Iteration 420, loss = 1435.75144683\n",
      "Iteration 421, loss = 1436.69371355\n",
      "Iteration 422, loss = 1434.82888415\n",
      "Iteration 423, loss = 1435.58629199\n",
      "Iteration 424, loss = 1435.43036582\n",
      "Iteration 425, loss = 1432.45939786\n",
      "Iteration 426, loss = 1434.96484852\n",
      "Iteration 427, loss = 1432.36515763\n",
      "Iteration 428, loss = 1431.18320984\n",
      "Iteration 429, loss = 1431.83740495\n",
      "Iteration 430, loss = 1432.21614786\n",
      "Iteration 431, loss = 1429.54076687\n",
      "Iteration 432, loss = 1428.04391102\n",
      "Iteration 433, loss = 1434.37123403\n",
      "Iteration 434, loss = 1430.25470293\n",
      "Iteration 435, loss = 1429.60283808\n",
      "Iteration 436, loss = 1431.02379848\n",
      "Iteration 437, loss = 1428.08733947\n",
      "Iteration 438, loss = 1427.05713461\n",
      "Iteration 439, loss = 1427.71446830\n",
      "Iteration 440, loss = 1426.67985918\n",
      "Iteration 441, loss = 1426.20209858\n",
      "Iteration 442, loss = 1424.13498205\n",
      "Iteration 443, loss = 1424.56329812\n",
      "Iteration 444, loss = 1425.20511596\n",
      "Iteration 445, loss = 1424.03962639\n",
      "Iteration 446, loss = 1421.95365279\n",
      "Iteration 447, loss = 1422.32528370\n",
      "Iteration 448, loss = 1423.02935934\n",
      "Iteration 449, loss = 1421.95172275\n",
      "Iteration 450, loss = 1421.38947328\n",
      "Iteration 451, loss = 1420.16294481\n",
      "Iteration 452, loss = 1420.78339910\n",
      "Iteration 453, loss = 1423.30011002\n",
      "Iteration 454, loss = 1421.56776843\n",
      "Iteration 455, loss = 1418.49431494\n",
      "Iteration 456, loss = 1420.47106092\n",
      "Iteration 457, loss = 1419.22178350\n",
      "Iteration 458, loss = 1414.87114212\n",
      "Iteration 459, loss = 1417.81810320\n",
      "Iteration 460, loss = 1418.78891317\n",
      "Iteration 461, loss = 1416.24569584\n",
      "Iteration 462, loss = 1414.33254084\n",
      "Iteration 463, loss = 1414.29292316\n",
      "Iteration 464, loss = 1415.34408459\n",
      "Iteration 465, loss = 1419.05276022\n",
      "Iteration 466, loss = 1414.95353357\n",
      "Iteration 467, loss = 1413.55297410\n",
      "Iteration 468, loss = 1413.00992725\n",
      "Iteration 469, loss = 1413.75230727\n",
      "Iteration 470, loss = 1411.67083996\n",
      "Iteration 471, loss = 1411.90029677\n",
      "Iteration 472, loss = 1409.81491759\n",
      "Iteration 473, loss = 1410.48999704\n",
      "Iteration 474, loss = 1410.72566204\n",
      "Iteration 475, loss = 1409.26557811\n",
      "Iteration 476, loss = 1411.37226105\n",
      "Iteration 477, loss = 1408.14932981\n",
      "Iteration 478, loss = 1407.44288654\n",
      "Iteration 479, loss = 1409.22073886\n",
      "Iteration 480, loss = 1408.73282055\n",
      "Iteration 481, loss = 1406.58937564\n",
      "Iteration 482, loss = 1407.28997501\n",
      "Iteration 483, loss = 1406.98817686\n",
      "Iteration 484, loss = 1408.18053879\n",
      "Iteration 485, loss = 1406.09744463\n",
      "Iteration 486, loss = 1404.14962946\n",
      "Iteration 487, loss = 1406.60275176\n",
      "Iteration 488, loss = 1405.33933105\n",
      "Iteration 489, loss = 1402.75948056\n",
      "Iteration 490, loss = 1404.11735637\n",
      "Iteration 491, loss = 1400.58997544\n",
      "Iteration 492, loss = 1407.42751767\n",
      "Iteration 493, loss = 1402.05794190\n",
      "Iteration 494, loss = 1401.46372140\n",
      "Iteration 495, loss = 1400.52232197\n",
      "Iteration 496, loss = 1402.66162381\n",
      "Iteration 497, loss = 1398.39900657\n",
      "Iteration 498, loss = 1399.58891662\n",
      "Iteration 499, loss = 1394.48645308\n",
      "Iteration 500, loss = 1399.91004560\n",
      "Iteration 501, loss = 1399.48928146\n",
      "Iteration 502, loss = 1398.17463445\n",
      "Iteration 503, loss = 1401.28536966\n",
      "Iteration 504, loss = 1396.35300517\n",
      "Iteration 505, loss = 1397.98765082\n",
      "Iteration 506, loss = 1396.57478418\n",
      "Iteration 507, loss = 1393.73344314\n",
      "Iteration 508, loss = 1396.71566363\n",
      "Iteration 509, loss = 1394.92825838\n",
      "Iteration 510, loss = 1395.23351408\n",
      "Iteration 511, loss = 1393.65930027\n",
      "Iteration 512, loss = 1393.13956627\n",
      "Iteration 513, loss = 1398.22588006\n",
      "Iteration 514, loss = 1395.65856302\n",
      "Iteration 515, loss = 1391.52575983\n",
      "Iteration 516, loss = 1390.02885889\n",
      "Iteration 517, loss = 1394.49367020\n",
      "Iteration 518, loss = 1390.87191735\n",
      "Iteration 519, loss = 1392.50825354\n",
      "Iteration 520, loss = 1388.76870172\n",
      "Iteration 521, loss = 1389.45421796\n",
      "Iteration 522, loss = 1394.25418879\n",
      "Iteration 523, loss = 1388.10311621\n",
      "Iteration 524, loss = 1392.61521370\n",
      "Iteration 525, loss = 1388.61599445\n",
      "Iteration 526, loss = 1389.44830015\n",
      "Iteration 527, loss = 1388.02702883\n",
      "Iteration 528, loss = 1388.63645689\n",
      "Iteration 529, loss = 1387.09358142\n",
      "Iteration 530, loss = 1387.32218894\n",
      "Iteration 531, loss = 1385.08188692\n",
      "Iteration 532, loss = 1383.82910105\n",
      "Iteration 533, loss = 1385.76183824\n",
      "Iteration 534, loss = 1385.60826276\n",
      "Iteration 535, loss = 1383.56760365\n",
      "Iteration 536, loss = 1382.17371100\n",
      "Iteration 537, loss = 1385.14122996\n",
      "Iteration 538, loss = 1381.48841614\n",
      "Iteration 539, loss = 1384.89300847\n",
      "Iteration 540, loss = 1380.24371569\n",
      "Iteration 541, loss = 1380.57773904\n",
      "Iteration 542, loss = 1381.27607563\n",
      "Iteration 543, loss = 1383.85257928\n",
      "Iteration 544, loss = 1380.85531587\n",
      "Iteration 545, loss = 1377.82310786\n",
      "Iteration 546, loss = 1378.45591857\n",
      "Iteration 547, loss = 1379.10600266\n",
      "Iteration 548, loss = 1377.93912138\n",
      "Iteration 549, loss = 1380.45621493\n",
      "Iteration 550, loss = 1376.63544482\n",
      "Iteration 551, loss = 1376.61185707\n",
      "Iteration 552, loss = 1378.30473690\n",
      "Iteration 553, loss = 1374.31214781\n",
      "Iteration 554, loss = 1374.73122738\n",
      "Iteration 555, loss = 1377.01613369\n",
      "Iteration 556, loss = 1375.02862640\n",
      "Iteration 557, loss = 1376.47468352\n",
      "Iteration 558, loss = 1377.24423357\n",
      "Iteration 559, loss = 1371.24616843\n",
      "Iteration 560, loss = 1377.14210124\n",
      "Iteration 561, loss = 1375.07421213\n",
      "Iteration 562, loss = 1371.67810302\n",
      "Iteration 563, loss = 1370.43765930\n",
      "Iteration 564, loss = 1368.08133211\n",
      "Iteration 565, loss = 1368.97098056\n",
      "Iteration 566, loss = 1371.89443359\n",
      "Iteration 567, loss = 1369.37532579\n",
      "Iteration 568, loss = 1370.74530363\n",
      "Iteration 569, loss = 1369.89529054\n",
      "Iteration 570, loss = 1367.09650109\n",
      "Iteration 571, loss = 1371.80490650\n",
      "Iteration 572, loss = 1366.92217091\n",
      "Iteration 573, loss = 1370.02761451\n",
      "Iteration 574, loss = 1368.88514975\n",
      "Iteration 575, loss = 1363.26036445\n",
      "Iteration 576, loss = 1367.46199466\n",
      "Iteration 577, loss = 1364.85467382\n",
      "Iteration 578, loss = 1365.41366184\n",
      "Iteration 579, loss = 1365.85900118\n",
      "Iteration 580, loss = 1366.73380492\n",
      "Iteration 581, loss = 1364.80089625\n",
      "Iteration 582, loss = 1364.79909511\n",
      "Iteration 583, loss = 1362.85786655\n",
      "Iteration 584, loss = 1365.54027175\n",
      "Iteration 585, loss = 1363.09747628\n",
      "Iteration 586, loss = 1364.90029377\n",
      "Iteration 587, loss = 1361.26285486\n",
      "Iteration 588, loss = 1361.04766850\n",
      "Iteration 589, loss = 1359.75034324\n",
      "Iteration 590, loss = 1361.26644990\n",
      "Iteration 591, loss = 1359.46034953\n",
      "Iteration 592, loss = 1358.00460249\n",
      "Iteration 593, loss = 1360.43971476\n",
      "Iteration 594, loss = 1357.83444475\n",
      "Iteration 595, loss = 1358.73057257\n",
      "Iteration 596, loss = 1360.42948107\n",
      "Iteration 597, loss = 1359.80945542\n",
      "Iteration 598, loss = 1358.38673089\n",
      "Iteration 599, loss = 1357.87132205\n",
      "Iteration 600, loss = 1359.02721311\n",
      "Iteration 601, loss = 1359.00705424\n",
      "Iteration 602, loss = 1358.66301779\n",
      "Iteration 603, loss = 1356.99482292\n",
      "Iteration 604, loss = 1350.96985555\n",
      "Iteration 605, loss = 1352.99286630\n",
      "Iteration 606, loss = 1354.99040739\n",
      "Iteration 607, loss = 1355.11992519\n",
      "Iteration 608, loss = 1352.85936643\n",
      "Iteration 609, loss = 1357.65050292\n",
      "Iteration 610, loss = 1353.35657906\n",
      "Iteration 611, loss = 1352.50704120\n",
      "Iteration 612, loss = 1352.52212284\n",
      "Iteration 613, loss = 1353.40667509\n",
      "Iteration 614, loss = 1354.09555076\n",
      "Iteration 615, loss = 1353.48773992\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(hidden_layer_sizes=[4, 4, 2, 1], max_iter=10000, verbose=True)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rede=MLPRegressor(hidden_layer_sizes=[4,4,2,1],\n",
    "                  activation='relu',\n",
    "                  verbose=True,\n",
    "                  max_iter=10000,\n",
    "                  solver=\"adam\")\n",
    "Rede.fit(Entradas,Saidas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score do treinamento:  0.4932102105042324\n",
      "R2 Score do teste:  0.5220604958686311\n"
     ]
    }
   ],
   "source": [
    "r2train=Rede.score(Xtrain, Ytrain)\n",
    "print(\"R2 Score do treinamento: \", r2train)\n",
    "r2test=Rede.score(Xtest, Ytest)\n",
    "print(\"R2 Score do teste: \", r2test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Ypred=Rede.predict(Xtest)\n",
    "plt.scatter(Ypred,Ytest)\n",
    "plt.xlim([-1000,1000])\n",
    "plt.ylim([-1000,1000])\n",
    "#plt.scatter(Xtest,Ypred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'finalized_model_sem_ganhos.sav'\n",
    "pickle.dump(Rede, open(filename, 'wb'))\n",
    " \n",
    "# some time later...\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6465708993511641\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(Xtest, Ytest)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4333fd1fe43e466f515c3a6ba90777eaed51e6920feedfbbb88898e3a37377d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
